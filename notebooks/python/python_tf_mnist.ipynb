{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"http://deeplearning4j.org/doc\">DL4J Documentation</a>\n",
    "<a target=\"_blank\" href=\"https://nd4j.org/doc/\">ND4J Documentation</a>\n",
    "<a target=\"_blank\" href=\"https://deeplearning4j.org/datavecdoc/\">DataVec (ETL) Documentation</a>\n",
    "SKIL also comes with support for <a target=\"_blank\" href=\"https://keras.io\">Keras</a> and <a target=\"_blank\" href=\"https://www.tensorflow.org/\">Tensorflow</a> with %pyspark\n",
    "For examples please see <a target=\"_blank\" href=\"https://github.com/SkymindIO/SKIL_Examples\">SKIL_Examples</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "class MyModel(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.x = tf.placeholder(tf.float32, [None, n_inputs], name=\"input0\")\n",
    "        self.y = tf.placeholder(tf.float32, [None, n_outputs])\n",
    "        \n",
    "        self.model = tf.layers.dense(self.x, n_inputs / 2, activation=tf.nn.sigmoid)\n",
    "        self.model = tf.layers.dense(self.model, n_outputs)\n",
    "        \n",
    "        self.output = tf.nn.softmax(self.model, name=\"output0\")\n",
    "        \n",
    "        self.loss = tf.losses.softmax_cross_entropy(onehot_labels=self.y, logits=self.model)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        \n",
    "        self.train_op = self.optimizer.minimize(loss=self.loss)\n",
    "\n",
    "model = MyModel(784, 10)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "all_saver = tf.train.Saver()\n",
    "all_saver.save(sess, \"/tmp/tfmodel/chkpt\")\n",
    "tf.train.write_graph(sess.graph_def, \"/tmp/tfmodel\", \"model.txt\", True)\n",
    "all_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "checkpoint = tf.train.get_checkpoint_state(\"/tmp/tfmodel\")\n",
    "input_checkpoint = checkpoint.model_checkpoint_path\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(model.train_op, feed_dict={model.x: batch_xs, model.y: batch_ys})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "sess.run(model.output, feed_dict={model.x: mnist.test.images[0,:].reshape(1, 784)})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph=\"/tmp/tfmodel/model.txt\",\n",
    "                          input_saver=\"\",\n",
    "                          input_checkpoint=input_checkpoint,\n",
    "                          output_graph=\"/tmp/tfmodel/model.pb\",\n",
    "                          input_binary=False,\n",
    "                          output_node_names=\"output0\",\n",
    "                          restore_op_name=\"save/restore_all\",\n",
    "                          filename_tensor_name=\"save/Const:0\",\n",
    "                          clear_devices=True,\n",
    "                          initializer_nodes=\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh ls /tmp/tfmodel/\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "import skil"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "skilContext = skil.SkilContext(sc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "help(skilContext.addModelToExperiment)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "path = str(skilContext.copyModel(z, \"/tmp/tfmodel/model.pb\", \"tensorflow\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "\n",
    "model_id = skilContext.addModelToExperiment(z, path, \"tfModel\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n",
    "model_id"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%pyspark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0 - Scala 2.11",
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
