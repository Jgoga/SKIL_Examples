{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tutorial demonstrate how to leverage SKIL from an external system (a raw Jupyter Notebook, in this case).\n",
    "\n",
    "## Goals\n",
    "This tutorial will target the following features of SKIL:\n",
    "1. Upload a trained model to the SKIL server\n",
    "2. Utilize the [skil-clients](https://github.com/SkymindIO/skil-clients) API ([Python](https://github.com/SkymindIO/skil-clients/tree/master/python) version) \n",
    "3. Utilize the \"Model Server\" API (from skil-clients) to:\n",
    "    - Create workspaces,\n",
    "    - Create experiments\n",
    "    - Adding models\n",
    "    - Adding evaluations\n",
    "    - Adding minibatches\n",
    "    - Adding examples\n",
    "    - Calculating evaluations through the model feedback endpoint\n",
    "4. Utilize the \"Deployments\" API (from skil-clients) to:\n",
    "    - Deploy a model\n",
    "    - Start a model to serve.\n",
    "5. Utilize the \"Predictions\" API (from skil-clients) to:\n",
    "    - Classify an image\n",
    "    \n",
    "We'll install TensorFlow to train a basic MNIST model. Then we'll upload that model to the SKIL server. Later on we'll add the model into an experiment in a workspace and then add evaluations to it based on the training and test results obtain through the TensorFlow model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing TensorFlow\n",
    "You can skip this step if you already have TensorFlow installed and linked to your Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sx pip3 install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "This is a basic MNIST model taken from the TensorFlow [examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples) repo and modified a little for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def deepnn(x):\n",
    "  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "  Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "  Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "    dropout.\n",
    "  \"\"\"\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "  b_conv1 = bias_variable([32])\n",
    "  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "  b_conv2 = bias_variable([64])\n",
    "  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "  # Second pooling layer.\n",
    "  h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "  b_fc1 = bias_variable([1024])\n",
    "\n",
    "  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  W_fc2 = weight_variable([1024, 10])\n",
    "  b_fc2 = bias_variable([10])\n",
    "\n",
    "  y_conv = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name=\"output_node\") \n",
    "  return y_conv, keep_prob\n",
    " \n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Freezing the Tensorflow Model\n",
    "After training the TensorFlow model, you'll have to freeze it into a `.pb` file in order to be able to upload it to the SKIL server. By freezing the TensorFlow model, you're actually saving the weights and graph details into a single file which the SKIL server can understand and deploy for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data_directory\\train-images-idx3-ubyte.gz\n",
      "Extracting data_directory\\train-labels-idx1-ubyte.gz\n",
      "Extracting data_directory\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data_directory\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Training model...\n",
      "step 0, training accuracy 0.14\n",
      "test accuracy 0.606\n",
      "\n",
      "Saving checkpoint...\n",
      "\n",
      "Freezing graph...\n",
      "INFO:tensorflow:Restoring parameters from data_directory\\saved_checkpoint-0\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "Converted 8 variables to const ops.\n",
      "\n",
      "Graph frozen successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.training import saver as saver_lib\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "# Import data\n",
    "work_directory = 'data_directory'\n",
    "saver_write_version = 2\n",
    "\n",
    "mnist = input_data.read_data_sets(work_directory, one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "checkpoint_prefix = os.path.join(work_directory, \"saved_checkpoint\")\n",
    "checkpoint_meta_graph_file = os.path.join(work_directory,\n",
    "                                          \"saved_checkpoint.meta\")\n",
    "checkpoint_state_name = \"checkpoint_state\"\n",
    "input_graph_name = \"input_graph.pb\"\n",
    "output_graph_name = \"output_graph.pb\"\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "\n",
    "train_accuracy = 0\n",
    "test_accuracy = 0\n",
    "\n",
    "test_guesses = []\n",
    "test_correct = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(101):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 20 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  # These two string arrays will be used for the feedback endpoint at the end of the notebook\n",
    "  test_guesses = tf.argmax(y_conv, 1).eval(feed_dict={x: mnist.test.images, keep_prob: 1.0}).astype(str)\n",
    "  test_correct = tf.argmax(y_, 1).eval(feed_dict={y_: mnist.test.labels}).astype(str)\n",
    "  \n",
    "  test_accuracy = accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "  print('test accuracy %g' % test_accuracy)\n",
    "  \n",
    "  print(\"\\nSaving checkpoint...\")\n",
    "\n",
    "  saver = saver_lib.Saver(write_version=saver_write_version)\n",
    "  checkpoint_path = saver.save(\n",
    "      sess,\n",
    "      checkpoint_prefix,\n",
    "      global_step=0,\n",
    "      latest_filename=checkpoint_state_name)\n",
    "  graph_io.write_graph(sess.graph, work_directory, input_graph_name)\n",
    "\n",
    "  input_graph_path = os.path.join(work_directory, input_graph_name)\n",
    "  input_saver_def_path = \"\"\n",
    "  input_binary = False\n",
    "  output_node_names = \"output_node\"\n",
    "  restore_op_name = \"save/restore_all\"\n",
    "  filename_tensor_name = \"save/Const:0\"\n",
    "  output_graph_path = os.path.join(work_directory, output_graph_name)\n",
    "  clear_devices = False\n",
    "  input_meta_graph = checkpoint_meta_graph_file\n",
    "\n",
    "  print(\"\\nFreezing graph...\")\n",
    "    \n",
    "  freeze_graph.freeze_graph(\n",
    "        input_graph_path,\n",
    "        input_saver_def_path,\n",
    "        input_binary,\n",
    "        checkpoint_path,\n",
    "        output_node_names,\n",
    "        restore_op_name,\n",
    "        filename_tensor_name,\n",
    "        output_graph_path,\n",
    "        clear_devices,\n",
    "        \"\",\n",
    "        \"\",\n",
    "        input_meta_graph,\n",
    "        checkpoint_version=saver_write_version)\n",
    "  print(\"\\nGraph frozen successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing the skil-clients python API\n",
    "The [skil-clients](https://github.com/SkymindIO/skil-clients) API provides an easy way to utilize the SKIL's REST API in different languages. Here, the python version is demonstrated. Let's clone and install the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Cloning into 'skil-clients'...\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sx git clone https://github.com/SkymindIO/skil-clients.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Processing e:\\\\projects\\\\jupyter\\\\skil-clients\\\\python',\n",
       " 'Requirement already satisfied: urllib3>=1.15 in c:\\\\users\\\\shams\\\\miniconda3\\\\lib\\\\site-packages (from skil-client==1.1.0b0) (1.23)',\n",
       " 'Requirement already satisfied: six>=1.10 in c:\\\\users\\\\shams\\\\miniconda3\\\\lib\\\\site-packages (from skil-client==1.1.0b0) (1.11.0)',\n",
       " 'Requirement already satisfied: certifi in c:\\\\users\\\\shams\\\\miniconda3\\\\lib\\\\site-packages (from skil-client==1.1.0b0) (2018.4.16)',\n",
       " 'Requirement already satisfied: python-dateutil in c:\\\\users\\\\shams\\\\miniconda3\\\\lib\\\\site-packages (from skil-client==1.1.0b0) (2.7.3)',\n",
       " 'Building wheels for collected packages: skil-client',\n",
       " '  Running setup.py bdist_wheel for skil-client: started',\n",
       " \"  Running setup.py bdist_wheel for skil-client: finished with status 'done'\",\n",
       " '  Stored in directory: C:\\\\Users\\\\shams\\\\AppData\\\\Local\\\\Temp\\\\pip-ephem-wheel-cache-8i8v23ct\\\\wheels\\\\06\\\\79\\\\b3\\\\e5006d523ef08c96f7913ea1acbcbd07dcd81e6ef3a31e8c39',\n",
       " 'Successfully built skil-client',\n",
       " 'Installing collected packages: skil-client',\n",
       " '  Found existing installation: skil-client 1.1.0b0',\n",
       " '    Uninstalling skil-client-1.1.0b0:',\n",
       " '      Successfully uninstalled skil-client-1.1.0b0',\n",
       " 'Successfully installed skil-client-1.1.0b0']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sx pip install ./skil-clients/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Authenticating the Model Server and General API instances \n",
    "Let's create the necessary API instances for utilizing them for the REST services. \n",
    "\n",
    "### Note\n",
    "Model server and the general API (deployments, predictions) run on different ports. By default the Model Server runs on port `9100` and the other APIs are at port `9008`. In the code section below, the Model Server API would be accessable through the variable `api_instance_mh` and the other REST requests will be accessable through the variable `api_instance` as will be demonstrated soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with SKIL API...\n",
      "{'token': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJTa2lsVXNlciIsInN1YiI6IntcInVzZXJJZFwiOlwiYWRtaW5cIixcInVzZXJOYW1lXCI6XCJhZG1pblwiLFwicGFzc3dvcmRcIjpcImFkbWluXCIsXCJyb2xlXCI6XCJhZG1pblwiLFwic2NvcGVcIjpcImFkbWluXCJ9IiwiaXNzIjoiU2tpbEF1dGhNYW5hZ2VyIiwiZXhwIjoxNTMxMDUyNTE1LCJpYXQiOjE1MzA5NjYxMTV9.BusbjTQktRz7x0RftVrh6KU8xG_s1PC8oRwLCTZ0WFc'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import unittest\n",
    "import uuid\n",
    "\n",
    "import numpy\n",
    "import skil_client\n",
    "from skil_client import *\n",
    "from skil_client.rest import ApiException\n",
    "\n",
    "debug = False\n",
    "\n",
    "host = \"localhost\" # Rename this to the host you are using \n",
    "\n",
    "config = Configuration()\n",
    "config.host = \"{}:9008\".format(host)  # change this if you're using a different port number for the general API!\n",
    "config.debug = debug\n",
    "api_client = ApiClient(configuration=config)\n",
    "unique_id = str(uuid.uuid4())[:8]\n",
    "# create an instance of the API class\n",
    "api_instance = skil_client.DefaultApi(api_client=api_client)\n",
    "\n",
    "config_mh = Configuration()\n",
    "config_mh.host = \"{}:9100\".format(host)  # change this if you're using a different port number for the model server!\n",
    "config_mh.debug = debug\n",
    "api_client_mh = ApiClient(configuration=config_mh)\n",
    "# create an instance of the Model History API class\n",
    "api_instance_mh = skil_client.DefaultApi(api_client=api_client_mh)\n",
    "\n",
    "# authenticate\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "try:\n",
    "    print(\"Authenticating with SKIL API...\")\n",
    "    credentials = skil_client.Credentials(user_id=\"admin\", password=\"admin\") # Update this with the ID and password you're using for your SKIL server\n",
    "    token = api_instance.login(credentials)\n",
    "    pp.pprint(token)\n",
    "    # add credentials to config\n",
    "    config.api_key['authorization'] = token.token\n",
    "    config.api_key_prefix['authorization'] = \"Bearer\"\n",
    "    # for model history\n",
    "    config_mh.api_key['authorization'] = token.token\n",
    "    config_mh.api_key_prefix['authorization'] = \"Bearer\"\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DefaultApi->login: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the Frozen Model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model, please wait...\n",
      "{'file_upload_response_list': [{'file_content': None,\n",
      "                                'file_name': 'output_graph.pb',\n",
      "                                'key': 'file',\n",
      "                                'path': '/opt/skil/plugins/files/MODEL/output_graph.pb',\n",
      "                                'status': 'uploaded',\n",
      "                                'type': None}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading model, please wait...\")\n",
    "modelFile = os.path.join(work_directory, output_graph_name)\n",
    "uploads = api_instance.upload(file=modelFile)\n",
    "pp.pprint(uploads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the model file path\n",
    "This will give the path of the model file uploaded and stored on the server. This will be without the file schema (`file://` or `hdfs://`). So, this will have to be added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'file:///opt/skil/plugins/files/MODEL/output_graph.pb'\n"
     ]
    }
   ],
   "source": [
    "model_file_path = \"file://\" + uploads.file_upload_response_list[0].path\n",
    "pp.pprint(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Workspace/Model History\n",
    "Workspace and Model History means the same thing in SKIL's context. They are used to store the experiments for the models and their particular details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1530556526927,\n",
      " 'model_history_id': '25d1d742-2146-47c4-9386-93567d8a1833',\n",
      " 'model_labels': 'Jupyter, python, tensorflow',\n",
      " 'model_name': 'jupyter_workspace'}\n"
     ]
    }
   ],
   "source": [
    "add_model_history_response = api_instance_mh.add_model_history(\n",
    "    AddModelHistoryRequest(\"jupyter_workspace\", \n",
    "                           \"Jupyter, python, tensorflow\")\n",
    ")\n",
    "\n",
    "pp.pprint(add_model_history_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an Experiment to the Workspace\n",
    "You can add an experiment to the workspace without having to add the details of a Zeppelin Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_model_id': None,\n",
      " 'experiment_description': 'Leveraging SKIL from a Jupyter notebook',\n",
      " 'experiment_id': 'jupyter_experiment_12345',\n",
      " 'experiment_name': 'jupyter_experiment',\n",
      " 'input_data_uri': None,\n",
      " 'last_updated': None,\n",
      " 'model_history_id': '25d1d742-2146-47c4-9386-93567d8a1833',\n",
      " 'notebook_json': None,\n",
      " 'notebook_url': None,\n",
      " 'zeppelin_id': None}\n"
     ]
    }
   ],
   "source": [
    "model_history_id = add_model_history_response.model_history_id\n",
    "\n",
    "experiment_id = \"jupyter_experiment_12345\"\n",
    "\n",
    "add_experiment_response = api_instance_mh.add_experiment(\n",
    "    ExperimentEntity(\n",
    "        experiment_id=experiment_id,\n",
    "        experiment_name=\"jupyter_experiment\",\n",
    "        experiment_description=\"Leveraging SKIL from a Jupyter notebook\",\n",
    "        model_history_id=model_history_id\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(add_experiment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a model to an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"jupyter_mnist_model_12345\"\n",
    "\n",
    "add_model_instance_response = api_instance_mh.add_model_instance(\n",
    "    ModelInstanceEntity(\n",
    "        uri=model_file_path,\n",
    "        model_id=model_id,\n",
    "        model_labels=\"0, 1, 2, 3, 4, 5, 6, 7, 8, 9\", # Comma-separated MNIST labels (The format very important for the feedback endpoint)\n",
    "        model_name=\"Jupyter MNIST\",\n",
    "        model_version=\"1\",\n",
    "        created=int(round(time.time() * 1000)),\n",
    "        experiment_id=experiment_id\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(add_model_instance_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding evaluations to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.62,\n",
      " 'auc': 0.0,\n",
      " 'binary_threshold': 0.0,\n",
      " 'binary_thresholds': None,\n",
      " 'created': 1530560526440,\n",
      " 'eval_id': 'jupyter_model_eval_id_train',\n",
      " 'eval_name': 'MNIST Train Accuracy',\n",
      " 'eval_version': 1,\n",
      " 'evaluation': '',\n",
      " 'f1': 0.0,\n",
      " 'mean_absolute_error': 0.0,\n",
      " 'mean_relative_error': 0.0,\n",
      " 'model_instance_id': 'jupyter_mnist_model_12345',\n",
      " 'precision': 0.0,\n",
      " 'r2': 0.0,\n",
      " 'recall': 0.0,\n",
      " 'rmse': 0.0}\n",
      "{'accuracy': 0.6961,\n",
      " 'auc': 0.0,\n",
      " 'binary_threshold': 0.0,\n",
      " 'binary_thresholds': None,\n",
      " 'created': 1530560526440,\n",
      " 'eval_id': 'jupyter_model_eval_id_test',\n",
      " 'eval_name': 'MNIST Test Accuracy',\n",
      " 'eval_version': 2,\n",
      " 'evaluation': '',\n",
      " 'f1': 0.0,\n",
      " 'mean_absolute_error': 0.0,\n",
      " 'mean_relative_error': 0.0,\n",
      " 'model_instance_id': 'jupyter_mnist_model_12345',\n",
      " 'precision': 0.0,\n",
      " 'r2': 0.0,\n",
      " 'recall': 0.0,\n",
      " 'rmse': 0.0}\n"
     ]
    }
   ],
   "source": [
    "eval_id_train = \"jupyter_model_eval_id_train\"\n",
    "eval_id_test = \"jupyter_model_eval_id_test\"\n",
    "\n",
    "eval_created_time = int(round(time.time() * 1000))\n",
    "\n",
    "eval_response_train = api_instance_mh.add_evaluation_result(\n",
    "    EvaluationResultsEntity(\n",
    "        evaluation=\"\",\n",
    "        created=eval_created_time,\n",
    "        eval_name=\"MNIST Train Accuracy\",\n",
    "        model_instance_id=model_id,\n",
    "        accuracy=train_accuracy,\n",
    "        eval_id=eval_id_train,\n",
    "        eval_version=1\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(eval_response_train)\n",
    "\n",
    "eval_response_test = api_instance_mh.add_evaluation_result(\n",
    "    EvaluationResultsEntity(\n",
    "        evaluation=\"\",\n",
    "        created=eval_created_time,\n",
    "        eval_name=\"MNIST Test Accuracy\",\n",
    "        model_instance_id=model_id,\n",
    "        accuracy=test_accuracy,\n",
    "        eval_id=eval_id_test,\n",
    "        eval_version=2\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(eval_response_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a minibatch an examples for a model's evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_version': 0,\n",
      " 'eval_id': 'jupyter_model_eval_id_test',\n",
      " 'eval_version': 2,\n",
      " 'mini_batch_id': 'test_minibatch'}\n",
      "{'batch_size': 10000,\n",
      " 'minibatch': {'batch_version': 0,\n",
      "               'eval_id': 'jupyter_model_eval_id_test',\n",
      "               'eval_version': 2,\n",
      "               'mini_batch_id': 'test_minibatch'}}\n"
     ]
    }
   ],
   "source": [
    "minibatch_id = \"test_minibatch\"\n",
    "minibatch_size = 10000\n",
    "\n",
    "minibatch = MinibatchEntity(\n",
    "    mini_batch_id=minibatch_id,\n",
    "    batch_version=0,\n",
    "    eval_id=eval_id_test, # Evaluation id and evaluation version should match here\n",
    "    eval_version=2\n",
    "\n",
    ")\n",
    "\n",
    "minibatch_response = api_instance_mh.add_minibatch(\n",
    "    minibatch\n",
    ")\n",
    "\n",
    "pp.pprint(minibatch_response)\n",
    "\n",
    "examples_response = api_instance_mh.add_example_for_batch(\n",
    "    AddExampleRequest(\n",
    "        minibatch=minibatch,\n",
    "        batch_size=minibatch_size\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(examples_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the feedback endpoint to calculate and save a model's evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = api_instance_mh.add_model_feedback(\n",
    "    ModelFeedBackRequest(\n",
    "        batch_id=minibatch_id,\n",
    "        guesses=test_guesses.tolist(),\n",
    "        correct=test_correct.tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': {'knn': [], 'models': [], 'transforms': []},\n",
      " 'deployment_slug': 'deployment_jupyter',\n",
      " 'id': '23',\n",
      " 'name': 'deployment_jupyter',\n",
      " 'status': 'Not Deployed'}\n"
     ]
    }
   ],
   "source": [
    "deployment_name = \"deployment_jupyter\"\n",
    "create_deployment_request = CreateDeploymentRequest(deployment_name)\n",
    "deployment_response = api_instance.deployment_create(create_deployment_request)\n",
    "\n",
    "pp.pprint(deployment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1530536502305,\n",
      " 'deployment_id': 23,\n",
      " 'extra_args': None,\n",
      " 'file_location': None,\n",
      " 'id': 21,\n",
      " 'jvm_args': None,\n",
      " 'labels_file_location': None,\n",
      " 'launch_policy': {'@class': 'io.skymind.deployment.launchpolicy.DefaultLaunchPolicy',\n",
      "                   'maxFailuresMs': 300000,\n",
      "                   'maxFailuresQty': 3},\n",
      " 'model_state': None,\n",
      " 'model_type': 'model',\n",
      " 'name': 'tf_model',\n",
      " 'scale': 1.0,\n",
      " 'state': 'stopped',\n",
      " 'sub_type': None,\n",
      " 'updated': None}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tf_model\"\n",
    "uris = [\"{}/model/{}/default\".format(deployment_name, model_name),\n",
    "        \"{}/model/{}/v1\".format(deployment_name, model_name)]\n",
    "\n",
    "deploy_model_request = ImportModelRequest(model_name,\n",
    "                                          1, \n",
    "                                          file_location=model_file_path,\n",
    "                                          model_type=\"model\",\n",
    "                                          uri=uris)\n",
    "model_deployment_response = api_instance.deploy_model(deployment_response.id, deploy_model_request)\n",
    "pp.pprint(model_deployment_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a Model to Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': 1530536502305,\n",
      " 'deployment_id': 23,\n",
      " 'extra_args': None,\n",
      " 'file_location': None,\n",
      " 'id': 21,\n",
      " 'jvm_args': None,\n",
      " 'labels_file_location': None,\n",
      " 'launch_policy': {'@class': 'io.skymind.deployment.launchpolicy.DefaultLaunchPolicy',\n",
      "                   'maxFailuresMs': 300000,\n",
      "                   'maxFailuresQty': 3},\n",
      " 'model_state': None,\n",
      " 'model_type': 'model',\n",
      " 'name': 'tf_model',\n",
      " 'scale': 1.0,\n",
      " 'state': 'started',\n",
      " 'sub_type': None,\n",
      " 'updated': 1530537509074}\n",
      "\n",
      "Start serving model...\n",
      "Model started successfully!\n"
     ]
    }
   ],
   "source": [
    "model_state_change_response = api_instance.model_state_change(deployment_response.id,\n",
    "                                                              model_deployment_response.id,\n",
    "                                                              SetState(\"start\"))\n",
    "pp.pprint(model_state_change_response)\n",
    "\n",
    "import time\n",
    "\n",
    "# Checking if the model is already started\n",
    "print(\"\\nStart serving model...\")\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Query the model state\n",
    "    model_state = api_instance.model_state_change(deployment_response.id, \n",
    "                                    model_deployment_response.id, \n",
    "                                    SetState(\"start\")).state\n",
    "    \n",
    "    if model_state == \"started\":\n",
    "      print(\"Model started successfully!\")\n",
    "      break\n",
    "    else:\n",
    "      print(\"wait...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying an image through the Served Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_response = api_instance.classify(\n",
    "    deployment_name=deployment_name, \n",
    "    model_name=model_name,\n",
    "    body=INDArray\n",
    "    (\n",
    "        shape=[784],\n",
    "        data=mnist.test.images[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "pp.pprint(classification_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
